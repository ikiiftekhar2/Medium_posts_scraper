{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0969dbcd658c8ead57f22ae213a82cab277c08c512098e96b698276deb007ef23",
   "display_name": "Python 3.7.7 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"gpt-3\"] #tags to scrape\n",
    "years = ['2020'] #years to scrape during\n",
    "months = ['06', '07'] #months to scrape during (every available day within the month will be scraped)\n",
    "textName = \"mediumText.txt\" #name of the output file\n",
    "\n",
    "#don't touch unless you need to\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticles(linkz):\n",
    "    bodyText = \"\"\n",
    "    req = Request(linkz,headers=hdr)\n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page)  \n",
    "    title = ''\n",
    "    title_data = soup.find(\"article\").find_all(\"h1\")\n",
    "    title = title + title_data[0].text\n",
    "    description_data = soup.find(\"article\").find_all(\"p\")\n",
    "    descData = []\n",
    "    for row in description_data:\n",
    "        descData.append(row.text)\n",
    "    descData.pop(0)\n",
    "    description = ''\n",
    "    for c in descData:\n",
    "        description = description + c\n",
    "    tag_lists = soup.findAll(\"ul\")\n",
    "    tag_lists.pop(0)\n",
    "    tagsz = []\n",
    "    for ul in tag_lists:\n",
    "        for li in ul.findAll('li'):\n",
    "            tagsz.append(li.text)\n",
    "    return [title,description,tagsz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeLinksToArticles(tag, years, months):\n",
    "    startLink = \"https://medium.com/tag/\"+tag+\"/archive/\"\n",
    "    articleLinks = []\n",
    "    for y in years:\n",
    "        yearLink = startLink + y\n",
    "        for m in months:\n",
    "            monLink = yearLink + \"/\" + m\n",
    "            #open the month link and scrape all valid days (days w/ link) into drive\n",
    "            req = Request(monLink,headers=hdr)\n",
    "            page = urlopen(req)\n",
    "            monSoup = BeautifulSoup(page)\n",
    "            try: #if there are days\n",
    "                allDays = list(monSoup.find(\"div\", {\"class\": \"col u-inlineBlock u-width265 u-verticalAlignTop u-lineHeight35 u-paddingRight0\"}).find_all(\"div\", {\"class\":\"timebucket\"}))\n",
    "                for a in allDays:\n",
    "                    try: #try to see if that day has a link\n",
    "                        dayLink = a.find(\"a\")['href']\n",
    "                        req = Request(dayLink,headers=hdr)\n",
    "                        page = urlopen(req)\n",
    "                        daySoup = BeautifulSoup(page)\n",
    "                        links = list(daySoup.find_all(\"div\", {\"class\": \"postArticle-readMore\"}))\n",
    "                        for l in links:\n",
    "                            articleLinks.append(l.find(\"a\")['href'])\n",
    "                    except: pass\n",
    "            except: #take the month's articles\n",
    "                links = list(monSoup.find_all(\"div\", {\"class\": \"postArticle-readMore\"}))\n",
    "                for l in links:\n",
    "                    articleLinks.append(l.find(\"a\")['href'])\n",
    "                print(\"issueHere\")\n",
    "    print(\"Article Links: \", len(articleLinks))\n",
    "    return articleLinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "issueHere\n"
     ]
    }
   ],
   "source": [
    "articleLinks = []\n",
    "for tag in tags: \n",
    "    articleLinks.extend(scrapeLinksToArticles(tag, years, months))\n",
    "articleLinks = set(articleLinks) #get rid of any duplicates\n",
    "articleLinksz = list(articleLinks)\n",
    "outPutText = open(textName, \"a+\", encoding='utf8')\n",
    "count = 0\n",
    "for art in articleLinksz:\n",
    "    print(count)\n",
    "    try:\n",
    "        outPutText.write(str(getArticles(art)))\n",
    "    except:\n",
    "        print(\"Skipped an error\")\n",
    "    count += 1    \n",
    "outPutText.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}